import numpy as np
import pandas as pd
import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

# Load best model
model_path = "models/best_model.h5"  # Update with actual saved model path
model = load_model(model_path)
model.summary()

# Load test dataset
with open("data/processed/X_test.pkl", "rb") as f: X_test = pickle.load(f)
with open("data/processed/y_test.pkl", "rb") as f: y_test = pickle.load(f)

# Ensure consistent input shape
X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=288, dtype='float32', padding='post', truncating='post')


# Generate predictions
y_prob_test = model.predict(X_test_padded)
y_pred_prob_test = y_prob_test[:, 0]  # Extract probability scores
y_pred_test = (y_pred_prob_test > 0.5).astype(int)

def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(title)
    plt.show()

cm = confusion_matrix(y_test, y_pred_test)
plot_confusion_matrix(cm, ['HIGH RISK', 'LOW RISK'], title='Confusion Matrix')

labels = [0, 1]  # Define labels (0: Low Risk, 1: High Risk)
result = []

for label in labels:
    precision, recall, f_score, support = precision_recall_fscore_support(np.array(y_test) == label, np.array(y_pred_test) == label)
    result.append([label, recall[0], recall[1], precision[1], f_score[1]])

df_metrics = pd.DataFrame(result, columns=["Label", "Specificity", "Recall", "Precision", "F-Score"])
print(df_metrics)


def bootstrap_metrics(y_true, y_pred_prob, num_samples=1000):
    """
    Compute confidence intervals using bootstrapping.
    """
    metrics_dict = {'AUC': [], 'Precision': [], 'Recall': [], 'F1-Score': []}
    
    for _ in range(num_samples):
        sample_indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_sample_true = np.array(y_true)[sample_indices]
        y_sample_pred_prob = np.array(y_pred_prob)[sample_indices]
        y_sample_pred = (y_sample_pred_prob > 0.5).astype(int)

        # Compute metrics
        fpr, tpr, _ = roc_curve(y_sample_true, y_sample_pred_prob)
        auc_score = auc(fpr, tpr)
        precision, recall, f1, _ = precision_recall_fscore_support(y_sample_true, y_sample_pred, average='binary')

        metrics_dict['AUC'].append(auc_score)
        metrics_dict['Precision'].append(precision)
        metrics_dict['Recall'].append(recall)
        metrics_dict['F1-Score'].append(f1)

    # Compute mean and confidence intervals
    results = {}
    for metric, values in metrics_dict.items():
        mean_val = np.mean(values)
        ci_lower, ci_upper = np.percentile(values, [2.5, 97.5])
        results[metric] = {'Mean': mean_val, '95% CI': (ci_lower, ci_upper)}

    return results

# Compute bootstrap metrics
metric_results = bootstrap_metrics(y_test, y_pred_prob_test)

# Print the results
for metric, values in metric_results.items():
    print(f"{metric}: Mean = {values['Mean']:.4f}, 95% CI = {values['95% CI'][0]:.4f} - {values['95% CI'][1]:.4f}")



def plot_precision_recall_curve(y_true, y_pred_prob):
    precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)
    plt.figure(figsize=(7, 5))
    plt.plot(recall, precision, marker='o', linestyle='-', label="Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.grid()
    plt.show()

plot_precision_recall_curve(y_test, y_pred_prob_test)

# ==============================
# ðŸ”¥ Save Results for Future Analysis
# ==============================
df_metrics.to_csv("results/model_test_metrics.csv", index=False)
with open("results/bootstrap_metrics.pkl", "wb") as f: pickle.dump(metric_results, f)
